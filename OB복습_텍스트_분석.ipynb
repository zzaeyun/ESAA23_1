{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMv8+q+t8LEYpYJSiLckWh5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzaeyun/ESAA23_1/blob/main/OB%EB%B3%B5%EC%8A%B5_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 분석 이해\n",
        "\n",
        "> 텍스트 분석: 비정형 데이터인 텍스트를 분석\n",
        "- 어떻게 피처 형태로 추출하고 추출된 피처에 의미있는 값을 부여한느가 하는 것이 매우 중요한 요소\n",
        "\n",
        "> 피처 벡터화 Feature Vectorization / 피처 추출 Feature Extraction\n",
        ": 텍스트를 word 기반의 피처로 추출 후 피처에 숫자 값을 부여하여 벡터값으로 표현\n",
        "  - BOW (Bag of Words)/ Word2Vect\n",
        "\n",
        "<br>\n",
        "\n",
        "### 텍스트 분석 수행 프로세스\n",
        "1. 텍스트 사전 준비작업(텍스트 전처리): 대/소문자 변경, 특수문자 삭제 등의 클렌징 / 토큰화 작업, stop word 제거, 어근추출 (stemming/lemmatization) 등의 정규화\n",
        "2. 피처 벡터화/추출: 사전 준비 작업으로 가공된 텍스트에서 피처를 추출하고 벡터값 할당: BOW와 Word2Vect\n",
        "  - BOW는 Count 기반과 TF-IDF 기반 벡터화\n",
        "3. ML 모델 수립 및 학습/예측/평가: 피처 벡터화된 데이터 세트에 ML 모델 적용하여 학습/예측 및 평가 수행\n",
        "\n",
        "<br>\n",
        "\n",
        "###파이썬 기반의 NLP, 텍스트 분석 패키지\n",
        "\n",
        "*수행 성능과 정확도, 신기술, 엔터프라이즈한 기능 지원 측면에서 부족한 부분이 있으므로 Genism과 SpaCy를 실제 업무에서 활용\n",
        "\n",
        "- NTLK: 파이썬의 가장 대표적인 NLP 패키지로 방대한 데이터 세트와 서브 모듈 보유하여 NLP의 거의 모든 영역을 커버\n",
        "- Gensim: 토픽 모델링 분야에서 가장 두각을 나타내는 패키지로 토픽 모델링을 쉽게 구현할 수 있는 기능을 제공 (Word2Vec 구현)\n",
        "- SpaCy: 뛰어난 수행 성능으로 최근 가장 주목받는 패키지\n"
      ],
      "metadata": {
        "id": "MsI1A9zslFse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화\n",
        "\n",
        "텍스트 정규화 작업: 텍스트 자체를 바로 피처로 만들 수 없으므로 텍스트를 가공하는 준비 작업\n",
        "- 클렌징\n",
        "- 토큰화\n",
        "- 필터링/스톱 워드 제거/철자 수정\n",
        "- Stemming\n",
        "- Lemmatization\n",
        "\n",
        "<br>\n",
        "\n",
        "###**클렌징**\n",
        ": 텍스트에서 분석에 방해되는 불필요한 문자, 기호 등을 사전에 제거\n",
        "ex) HTML, XML 태그나 특정 기호\n",
        "\n",
        "###**텍스트 토큰화**\n",
        "1. 문장을 분리하는 문장 토큰화\n",
        "2. 단어를 토큰으로 분리하는 단어 토큰화\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "- 문장 토큰화 sentence tokenization\n",
        "문장의 마침표(.), 개행문자(\\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리. 정규 표현식에 따른 문장 토큰화도 가능\n",
        "  - 각 문장이 가지는 시맨틱적인 의미가 중요한 요소로 사용될 때 사용\n",
        "  - NTLK의 sent_tokenize를 이용하여 토큰화 수행\n",
        "\n",
        "<br>\n",
        "\n",
        "- 단어 토큰화 word tokenization\n",
        "공백, 콤마(,), 마침표(.), 개행문자 등으로 단어를 분리. 정규 표현식을 이용한 다양한 유형의 토큰화 수행 가능\n",
        "  - 마침표나 개행문자와 같이 문장을 분리하는 구분자를 이용해 단어를 토큰화할 수 있으므로 BOW와 같이 단어의 순서가 중요하지 않은 경우 문장 토큰화는 수행하지 않고 단어 토큰화만 사용해도 충분\n",
        "  - NTLK의 word_tokenize() 활용\n",
        "\n",
        "<br>\n",
        "\n",
        "###**스톱 워드 제거**\n",
        "스톱 워드 Stop word: 분석에 큰 의미가 없는 단어 지칭\n",
        "- ex. is, the, a, will 등 문장을 구성하는 필수 문법 요소이지만 문맥적으로 큰 의미가 없는 단어\n",
        "- 사전에 제거하지 않으면 빈번하게 나타나기 때문에 오히려 중요한 단어로 인지될 수 있음\n",
        "- 언어별로 스톱 워드가 목록화되어 있음\n"
      ],
      "metadata": {
        "id": "FDzemVYCqa_2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eUO6ZH7j0_O",
        "outputId": "29d4c601-a111-4ed4-942e-8bc0ba16dca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 stop words 개수:  179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print('영어 stop words 개수: ', len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Stemming과 Lemmatization**\n",
        "문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 것\n",
        "\n",
        "- Lemmatization: 의미론적인 기반에서 단어의 원형을 찾음\n",
        "- Stemming: 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용해 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출\n",
        "  -  Lemmatization이 Stemming 보다 정확한 철자로 된 어근을 찾기때문에 변환에 더 오랜 시간을 필요로 함\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3nY1m5fjvod9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words - BOW\n",
        "문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델\n",
        "\n",
        "1. 문장1과 문장2에 모든 단어에서 중복을 제거하고 각 단어(feature 또는 term)를 칼럼 형태로 나열한 후 각 단어에 고유의 인덱스를 다음과 같이 부여\n",
        "2. 개별 문장에서 해당 단어가 나타내는 횟수를 각 단어에 기재\n",
        "\n",
        "<br>\n",
        "\n",
        "BOW 모델의 장점\n",
        "- 쉽고 빠른 구축\n",
        "- 활용도가 높음\n",
        "\n",
        "BOW 모델의 단점\n",
        "- 문맥 의미 반영 부족\n",
        "- 희소 행렬 문제 (희소성, 희소 행렬): 많은 문서에서 단어를 추출하면 매우 많은 단어가 칼럼으로 만들어지고 문서마다 서로 다른 단어로 구성되어 문서마다 나타나지 않음. 그런 경우 대부분의 값이 0으로 채워지는 희소 행렬의 형태를 띔\n",
        "\n",
        "<br>\n",
        "\n",
        "**BOW 피처 벡터화**\n",
        "\n",
        "머신러닝 알고리즘은 일반적으로 숫자형 피처를 데이터로 입력받아 동작하기 때문에 텍스트와 같은 데이터는 머신러닝 알고리즘에 바로 입력할 수 없음. 따라서 텍스트는 특정 의미를 가지는 숫자형 값인 벡터 값으로 변환해야 하는데, 이러한 변환을 피처 벡터화\n",
        "\n",
        "- BOW 모델에서 피처 벡터화를 수행한다는 것은 모든 문서에서 모든 단어를 칼럼 형태로 나열하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것\n",
        "- 예를 들어 M개의 텍스트 문서가 있고, 이 문서에서 모든 단어를 추출해 나열했을 때 N개의 단어가 있다고 가정하면 문서의 피처 벡터화를 수행하면 M개의 문서는 각각 N개의 값이 할당된 피처의 벡터 세트가 됨: M X N 개의 단어 피처로 이뤄진 행렬을 구성\n",
        "\n",
        "<br>\n",
        "\n",
        "BOW의 피처 벡터화 방식\n",
        "- 카운트 기반의 벡터화: 해당 단어가 나타나는 횟수\n",
        "- TF-IDF 기반의 벡터화: 자주 나타나는 단어에 높은 가중치를 주되, 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서는 패널티를 주는 방식\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### 사이킷런의 Count 및 TF-IDF 벡터화 구현: CountVectorizer, TfidfVectorizer\n",
        "\n",
        "**Count Vectorizer**\n",
        "- 클래스는 카운트 기반의 벡터화를 구현한 클래스: 피처 벡터화 + 소문자 일괄 변환, 토큰화, 스톱 워드 필터리 등의 텍스트 전처리\n",
        "\n",
        "**CountVectorizer의 입력 파라미터**\n",
        "- max_df: 전체 문서에 걸쳐서 너무 높은 빈도수를 가지는 단어 피처를 제외하기 위한 파라미터\n",
        "- min_df: 전체 문서에 걸쳐서 너무 낮은 빈도수를 가지는 단어 피처를 제외하기 위한 파라미터\n",
        "  - 정수 값을 가지면 전체 문서에 걸쳐서 n번 이하로 나타나는 단어는 피처로 추출하지 않음\n",
        "  - 부동 소수점을 가지면 전체 문서에 걸쳐서 하위 n% 이하의 빈도수를 가지는 단어는 피처로 추출하지 않음\n",
        "- max_features: 추출하는 피처의 개수를 제한하며 정수로 값을 지정\n",
        "- stop_words: english로 지정하면 영어의 스톱 워드로 지정된 단어는 추출에서 제외\n",
        "- n_gram_range: BOW 모델의 단어 순서를 어느 정도 보강하기 위한 n_gram 범위를 설정. 튜플 형태로 (범위 최솟값, 범위 최댓값)을 지정\n",
        "- analyzer: 피처 추출을 수행한 단위를 지정\n",
        "- token_pattern: 토큰화를 수행하는 정규 표현식 패턴을 지정\n",
        "- tokenizer: 토큰화를 별도의 커스텀 함수로 이용시 적용\n",
        "\n",
        "<br>\n",
        "\n",
        "CountVectorizer 클래스 사용법\n",
        "1. 영어의 경우 모든 문자를 소문자로 변경하는 등의 전처리 작업 수행\n",
        "2. 디폴트로 단어 기준으로 n_gram_range를 반영해 각 단어를 토킄ㄴ화\n",
        "3. 텍스트 정규화 수행\n"
      ],
      "metadata": {
        "id": "rR_FdMylxvv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BOW 벡터화를 위한 희소 행렬**\n",
        "- 희소 행렬은 너무 많은 불필요한 0값이 메모리 공간에 할당되어 메모리 공간이 많이 필요하며, 행렬의 크기가 커서 연산 시에도 데이터 액세스를 위한 시간이 많이 소모\n",
        "- 이러한 희소 행렬을 물리적으로 적은 메모리 공간을 차지할 수 있도록 변환해야 함\n",
        "  - COO형식\n",
        "  - CSR 형식\n",
        "\n",
        "<br>\n",
        "\n",
        "### **희소 행렬-COO형식**\n",
        "COO(Coordinate: 좌표) 형식은 0이 아닌 데이터만 별도의 데이터 배열에 저장하고, 그 데이터가 가르키는 행과 열의 윛치를 별도의 배열로 저장하는 방식\n",
        "- 주로 사이파이 Scipy를 이용\n",
        "\n",
        "###**희소 행렬 - CSR 형식**\n",
        "CSR(Compressed Sparse Row) 형식은 COO 형식이 행과 열의 위치를 나타내기 위해서 반복적인 위치 데이터를 사용해야 하는 문제점을 해결한 방식"
      ],
      "metadata": {
        "id": "NfVJknh3DzMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##감성분석\n",
        "\n",
        "감성분석(Sentiment Analysis)\n",
        "- 문서의 주관적인 감성/의견/감정/기분 등을 파악하기 위한 방법으로 소셜 미디어, 여론조사, 온라인 리뷰, 피드백 등 다양한 분야에서 활용\n",
        "- 문서 내 텍스트가 나타내는 여러 가지 주관적인 단어와 문맥을 기반으로 감성(Sentiment) 수치를 계산하는 방법\n",
        "  - 감성 지수는 긍정 감성 지수와 부정 감성 지수로 구성되며 이들 지수를 합산해 긍정 감성 또는 부정 감성을 결정\n",
        "\n",
        "  <br>\n",
        "\n",
        "**지도학습**\n",
        "- 학습 데이터와 타깃 레이블 값을 기반으로 감성 분석 학습을 수행한 뒤 이를 기반으로 다른 데이터의 감성 분석을 예측하는 방법으로 일반적인 텍스트 기반의 분류와 거의 동일\n",
        "**비지도학습**\n",
        "- 'Lexicon'이라는 일종의 감성 어휘 사전을 이용.\n",
        "- Lexicon은 감성 분석을 위한 용어와 문맥에 대한 다양한 정보를 가지고 있으며, 이를 이용해 문서의 긍정적, 부정적 감성 여부를 판단\n",
        "\n",
        "\n",
        "### 비지도학습 기반 감성 분석\n",
        "- Lexicon을 기반: 한글 지원X\n",
        "- Lexicon은 일반적으로 어휘집을 의미. 여기서는 주로 감성만을 분석하기 위해 지원하는 감성 어휘 사전: 감성 사전\n",
        "  - 감성 사전은 긍정(Positive) 감성 또는 부정(Negative)감성의 정도를 의미하는 수치를 가지고 있음: 이를 감성 지수(Polarity score)라고 함\n",
        "  - 감성 지수는 단어의 위치나 주변 단어, 문맥, POS(Part of Speech) 등을 참고하여 결정\n",
        "- NLTK 패키지\n",
        "  - 감성 사전을 구현한 패키지\n",
        "  - Lexicon 모듈을 포함하여 많은 서브 모듈을 가지고 있음\n",
        "\n",
        "<br>\n",
        "\n",
        "**WordNet**\n",
        "- 방대한 영어 어휘 사전\n",
        "  - 단순한 어휘 사전이 아닌 시맨틱 분석을 제공하는 어휘 사전\n",
        "  - **시맨틱(semantic)**: 문맥상 의미\n",
        "- 다양한 상황에서 같은 어휘라도 다르게 사용되는 어휘의 시맨틱 정보를 제공하며, 이를 위해 각각의 품사로 구성된 개별 단어를 Synset(Sets of cogmitive synonyms)이라는 개념을 이용하여 표현\n",
        "  - Synset은 단순히 하나의 단어가 아니라 그 단어가 가지는 문맥, 시맨틱 정보를 제공하는 WordNet의 핵심 개념\n",
        "\n",
        "<br>\n",
        "\n",
        "**NLTK를 포함한 대표적인 감성 사전**\n",
        "- SentiWordNet: NLTK 패키지의 WordNet과 유사하게 감성 단어 전용의 WordNet을 구현한 것\n",
        "  - 3가지 감성 점수 할당: 긍정 감성 지수, 부정 감성 지수, 객관성 지수\n",
        "- VADER: 주로 소셜 미디어의 텍스트에 대한 감성 분석을 제공하기 위한 패키지. 뛰어난 감성 분석 결과를 제공하며, 비교적 빠른 수행 시간을 보장해 대용량 텍스트 데이터에 잘 사용되는 패키지\n",
        "- Pattern: 예측 성능 측면에서 가장 주목받는 패키지. 아쉽께도 파이썬 3.X 버전에서 호환 불가능\n",
        "\n"
      ],
      "metadata": {
        "id": "C9x_S1zOENyy"
      }
    }
  ]
}