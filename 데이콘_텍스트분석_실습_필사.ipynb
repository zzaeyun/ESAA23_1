{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTnUSqDiuDvDNnZZxaRr5+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzaeyun/ESAA23_1/blob/main/%EB%8D%B0%EC%9D%B4%EC%BD%98_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%B6%84%EC%84%9D_%EC%8B%A4%EC%8A%B5_%ED%95%84%EC%82%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 데이콘 텍스트분석 실습 필사\n",
        "\n",
        "#1. Facebook의 fasttext를 이용한 text classification\n",
        "\n",
        "https://dacon.io/competitions/official/235670/codeshare/1840?page=undefined&dtype=recent&ptype=undefined\n"
      ],
      "metadata": {
        "id": "rkxflgWOQV7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xnq5M9CDRwc7",
        "outputId": "aa15eb87-1b7d-4171-a98c-d65c852eda3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from fasttext) (67.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from fasttext) (1.22.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp39-cp39-linux_x86_64.whl size=4395598 sha256=c78e11cc20e3ad33aaf2df9e03ce849e028308ecf3dd8142d7fec192f3458534\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/57/bc/1741406019061d5664914b070bd3e71f6244648732bc96109e\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LPRUIhDSQL_4"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train=pd.read_csv('/train.csv')\n",
        "\n",
        "file=open('fasttexttrain.txt','w+')\n",
        "\n",
        "for i in train.index:\n",
        "  line='__label__' +str(train['author'][i])+''+train['text'][i]\n",
        "  file.write(line+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Classification\n",
        "- input             # training file path (required)\n",
        "- model             # unsupervised fasttext model {cbow, skipgram} [skipgram]\n",
        "- lr                # learning rate [0.05]\n",
        "- dim               # size of word vectors [100]\n",
        "- ws                # size of the context window [5]\n",
        "- epoch             # number of epochs [5]\n",
        "- minCount          # minimal number of word occurences [5]\n",
        "- minn              # min length of char ngram [3]\n",
        "- maxn              # max length of char ngram [6]\n",
        "- neg               # number of negatives sampled [5]\n",
        "- wordNgrams        # max length of word ngram [1]\n",
        "- loss              # loss function {ns, hs, softmax, ova} [ns]\n",
        "- bucket            # number of buckets [2000000]\n",
        "- thread            # number of threads [number of cpus]\n",
        "- lrUpdateRate      # change the rate of updates for the - - learning rate [100]\n",
        "- t                 # sampling threshold [0.0001]\n",
        "- verbose           # verbose [2]\n"
      ],
      "metadata": {
        "id": "9N2QMIOxSP4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_clf_model = fasttext.train_supervised('fasttexttrain.txt', epoch=30, minCount=2, maxn=10, verbose=0)\n",
        "print(text_clf_model.words)\n",
        "print(text_clf_model.labels)"
      ],
      "metadata": {
        "id": "hhYeT2jsRnxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reuslt = text_clf_model.predict(\"He was almost choking. There was so much, so much he wanted to say, but strange exclamations were all that came from his lips. The Pole gazed fixedly at him, at the bundle of notes in his hand; looked at odin, and was in evident perplexity.\", k=5)\n",
        "print(reuslt)"
      ],
      "metadata": {
        "id": "xezmaG0whVtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/test_x.csv')\n",
        "submission = pd.read_csv('/sample_submission.csv', index_col=False)"
      ],
      "metadata": {
        "id": "PeksZ6O_hTn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in test.index:\n",
        "    lable, proba = text_clf_model.predict(test['text'][i], k=5)\n",
        "    for la, pr in zip(lable, proba):\n",
        "        if '__label__0' == la:\n",
        "            submission.loc[i, '0'] = pr\n",
        "        elif '__label__1' == la:\n",
        "            submission.loc[i, '1'] = pr\n",
        "        elif '__label__2' == la:\n",
        "            submission.loc[i, '2'] = pr\n",
        "        elif '__label__3' == la:\n",
        "            submission.loc[i, '3'] = pr\n",
        "        elif '__label__4' == la:\n",
        "            submission.loc[i, '4'] = pr\n",
        "    # submission.loc[i, '0'] = proba[lable.loc('__label__0')]\n",
        "    # submission.loc[i, '1'] = proba[4]\n",
        "    # submission.loc[i, '2'] = proba[2]\n",
        "    # submission.loc[i, '3'] = proba[0]\n",
        "    # submission.loc[i, '4'] = proba[3]\n",
        "\n",
        "submission.to_csv('result5_fasttext.csv', index=False)\n",
        "print('end')"
      ],
      "metadata": {
        "id": "3VHqUEO2Sr7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. NLP 전처리\n",
        "\n",
        "https://dacon.io/competitions/official/235670/codeshare/1823?page=2&dtype=recent"
      ],
      "metadata": {
        "id": "KPVjzJCYSw0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NLP Preprocessing**\n",
        "- NLP 전처리 과정은 주로 모델의 입력인 단어,문장,문서의 vector를 만들기 전에 진행\n",
        "- NLP 전처리 과정\n",
        "  1. 데이터를 불러온 후 각 신문기사들을 눈으로 확인하며 특수문자, 불용어 그리고 문장 구조에 대한 감을 잡습니다.\n",
        "  2. 문제의 목적과 분석자의 재량에 따라 불용어를 설정하고 리스트에 저장합니다. \n",
        "  3. 불용어 이외의 특수 문자들을 제거합니다. \n",
        "  4. 형태소 분석을 통해 문장을 형태소 단위의 토큰으로 분리합니다. 이때 내가 설정한 불용어들을 결과로 반환해주는 형태소 분석기를 사용하셔야 합니다.\n",
        "  5. 형태소 단위의 토큰들을 기반으로 리스트에 저장된 불용어를 제거합니다."
      ],
      "metadata": {
        "id": "fattf1ycS_Sj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. 형태소 분석(Stemming)**\n",
        "- 형태소 분석이란 단어나 문장의 언어적 속성을 파악하는 것을 의미합니다. 보통 품사의 태깅(PoS)을 통해 이루어지며 한국어 형태소 분석을 위해 Konlpy 패키지에 있는 다양한 함수를 이용하여 진행\n",
        "- 형태소 분석을 하는 이유는 주로 형태소 단위로 의미있는 단어 를 가져가고 싶거나 품사 태깅을 통해 형용사나 명사를 추출하고 싶을 때 많이 이용\n",
        "- **형태소 분석은 어쩌면 모델링보다 성능에 더 중요한 영향을 미치는 아주 중요한 과정**"
      ],
      "metadata": {
        "id": "ae0zKopQUEgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "epaoD5K7U4xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8a8fe0-2644-4c4b-caf2-c2314c7dbcec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from konlpy) (4.9.2)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.9/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1-1. Kkma()"
      ],
      "metadata": {
        "id": "Pw5WvAqOVFmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "\n",
        "sentence = '데이콘에서 다양한 컴피티션을 즐기면서 실력있는 데이터 분석가로 성장하세요!!.'\n",
        "\n",
        "print(\"형태소 단위로 문장 분리\")\n",
        "print(\"----------------------\")\n",
        "print(kkma.morphs(sentence))\n",
        "print(\" \")\n",
        "print(\"문장에서 명사 추출\")\n",
        "print(\"----------------------\")\n",
        "print(kkma.nouns(sentence))\n",
        "print(\" \")\n",
        "print(\"품사 태킹(PoS)\")\n",
        "print(\"----------------------\")\n",
        "print(kkma.pos(sentence))"
      ],
      "metadata": {
        "id": "ijQd9ojRS2G5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2583035a-5671-455b-bcf3-a21c43803d94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "형태소 단위로 문장 분리\n",
            "----------------------\n",
            "['데이', '콘', '에서', '다양', '하', 'ㄴ', '컴피티션', '을', '즐기', '면서', '실력', '있', '는', '데이터', '분석가', '로', '성장', '하', '세요', '!!', '.']\n",
            " \n",
            "문장에서 명사 추출\n",
            "----------------------\n",
            "['데이', '데이콘', '콘', '다양', '컴피티션', '실력', '데이터', '분석가', '성장']\n",
            " \n",
            "품사 태킹(PoS)\n",
            "----------------------\n",
            "[('데이', 'NNG'), ('콘', 'NNG'), ('에서', 'JKM'), ('다양', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('컴피티션', 'UN'), ('을', 'JKO'), ('즐기', 'VV'), ('면서', 'ECE'), ('실력', 'NNG'), ('있', 'VV'), ('는', 'ETD'), ('데이터', 'NNG'), ('분석가', 'NNG'), ('로', 'JKM'), ('성장', 'NNG'), ('하', 'XSV'), ('세요', 'EFN'), ('!!', 'SW'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1-2. Okt()"
      ],
      "metadata": {
        "id": "n-82hjxaVJcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "Okt = Okt()\n",
        "\n",
        "sentence = '데이콘에서 다양한 컴피티션을 즐기면서 실력있는 데이터 분석가로 성장하세요!!.'\n",
        "\n",
        "print(\"형태소 단위로 문장 분리\")\n",
        "print(\"----------------------\")\n",
        "print(Okt.morphs(sentence))\n",
        "print(\" \")\n",
        "print(\"문장에서 명사 추출\")\n",
        "print(\"----------------------\")\n",
        "print(Okt.nouns(sentence))\n",
        "print(\" \")\n",
        "print(\"품사 태킹(PoS)\")\n",
        "print(\"----------------------\")\n",
        "print(Okt.pos(sentence))"
      ],
      "metadata": {
        "id": "sats1w3zVCBE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ed8cd4-7641-4677-912f-7096f1c581ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "형태소 단위로 문장 분리\n",
            "----------------------\n",
            "['데', '이콘', '에서', '다양한', '컴피티션', '을', '즐기면서', '실력', '있는', '데이터', '분석', '가로', '성장하세요', '!!.']\n",
            " \n",
            "문장에서 명사 추출\n",
            "----------------------\n",
            "['데', '이콘', '컴피티션', '실력', '데이터', '분석', '가로']\n",
            " \n",
            "품사 태킹(PoS)\n",
            "----------------------\n",
            "[('데', 'Noun'), ('이콘', 'Noun'), ('에서', 'Josa'), ('다양한', 'Adjective'), ('컴피티션', 'Noun'), ('을', 'Josa'), ('즐기면서', 'Verb'), ('실력', 'Noun'), ('있는', 'Adjective'), ('데이터', 'Noun'), ('분석', 'Noun'), ('가로', 'Noun'), ('성장하세요', 'Adjective'), ('!!.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1-3. Mecab()"
      ],
      "metadata": {
        "id": "ObCJuJFPVN_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from konlpy.tag import Mecab \n",
        "#Mecab  = Mecab ()\n",
        "\n",
        "#sentence = '데이콘에서 다양한 컴피티션을 즐기면서 실력있는 데이터 분석가로 성장하세요!!.'\n",
        "\n",
        "#print(\"형태소 단위로 문장 분리\")\n",
        "#print(\"----------------------\")\n",
        "#print(Mecab .morphs(sentence))\n",
        "#print(\" \")\n",
        "#print(\"문장에서 명사 추출\")\n",
        "#print(\"----------------------\")\n",
        "#print(Mecab .nouns(sentence))\n",
        "#print(\" \")\n",
        "#print(\"품사 태킹(PoS)\")\n",
        "#print(\"----------------------\")\n",
        "#print(Mecab .pos(sentence))"
      ],
      "metadata": {
        "id": "0UsDVt8kVWQM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**형태소 분석기**\n",
        "- Mecab: 굉장히 속도가 빠르면서도 좋은 분석 결과를 보여준다.\n",
        "- Komoran: 댓글과 같이 정제되지 않은 글에 대해서 먼저 사용해보면 좋다.(오탈자를 어느정도 고려해준다.)\n",
        "- Kkma: 분석 시간이 오래걸리기 때문에 잘 이용하지 않게 된다.\n",
        "- Okt: 품사 태깅 결과를 Noun, Verb등 알아보기 쉽게 반환해준다.\n",
        "- khaiii: 카카오에서 가장 최근에 공개한 분석기, 성능이 좋다고 알려져 있으며 다양한 실험이 필요하다."
      ],
      "metadata": {
        "id": "vQImuDXRVfdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. 표제어 추출(Lemmatization)**"
      ],
      "metadata": {
        "id": "uj57iOQnWCiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "\n",
        "sentence = '성장했었다.'\n",
        "\n",
        "print(\"품사 태킹(PoS)\")\n",
        "print(\"----------------------\")\n",
        "print(kkma.pos(sentence))"
      ],
      "metadata": {
        "id": "7y-jKAAMWBKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ecd8fb7-2c84-4fa7-80c2-c4db7ea5e114"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "품사 태킹(PoS)\n",
            "----------------------\n",
            "[('성장', 'NNG'), ('하', 'XSV'), ('었', 'EPT'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = '성장하였었다.'\n",
        "\n",
        "print(\"품사 태킹(PoS)\")\n",
        "print(\"----------------------\")\n",
        "print(kkma.pos(sentence))"
      ],
      "metadata": {
        "id": "wi1eqoQsWXQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7bd484a-77b2-400d-8e50-c7797d238181"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "품사 태킹(PoS)\n",
            "----------------------\n",
            "[('성장', 'NNG'), ('하', 'XSV'), ('였', 'EPT'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. 불용어 제거(Stopwords removing)**\n",
        "\n",
        "- 불용어: 문장에서 큰 의미가 없다고 생각되는 단어, 글자들"
      ],
      "metadata": {
        "id": "7ap6m6W3WfcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from konlpy.tag import Okt\n",
        "tokenizer = Okt()\n",
        "def text_preprocessing(text,tokenizer):\n",
        "    \n",
        "    stopwords = ['을', '를', '이', '가', '은', '는']\n",
        "    \n",
        "    txt = re.sub('[^가-힣a-z]', ' ', text)\n",
        "    token = tokenizer.morphs(txt)\n",
        "    token = [t for t in token if t not in stopwords]\n",
        "        \n",
        "    return token\n",
        "\n",
        "ex_text = \"이번에 새롭게 개봉한 영화의 배우들은 모두 훌륭한 연기력과 아름다운 목소리를 갖고 있어!!\"\n",
        "example_pre= text_preprocessing(ex_text,tokenizer)"
      ],
      "metadata": {
        "id": "s3Dt_9y2Wfzy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 영어 소문자와 한글을 제외한 모든 문자를 제거\n",
        "2. Okt를 이용해 형태소 분석\n",
        "3. 형태소 분석기를 거쳐 나온 결과들 중 stopwords 리스트에 포함되지 않는 토큰만 token이라는 리스트에 반환"
      ],
      "metadata": {
        "id": "fq9PV_R6W0kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(example_pre)"
      ],
      "metadata": {
        "id": "n1YuzRDCW3xD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b48273-71a7-4379-b856-5a831cc02d82"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['이번', '에', '새롭게', '개봉', '한', '영화', '의', '배우', '들', '모두', '훌륭한', '연기력', '과', '아름다운', '목소리', '갖고', '있어']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 대회 적용"
      ],
      "metadata": {
        "id": "O5yxfoB9W9LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing(text_list):\n",
        "    \n",
        "    stopwords = ['을', '를', '이', '가', '은', '는', 'null'] #불용어 설정\n",
        "    tokenizer = Okt() #형태소 분석기 \n",
        "    token_list = []\n",
        "    \n",
        "    for text in text_list:\n",
        "        txt = re.sub('[^가-힣a-z]', ' ', text) #한글과 영어 소문자만 남기고 다른 글자 모두 제거\n",
        "        token = tokenizer.morphs(txt) #형태소 분석\n",
        "        token = [t for t in token if t not in stopwords or type(t) != float] #형태소 분석 결과 중 stopwords에 해당하지 않는 것만 추출\n",
        "        token_list.append(token)\n",
        "        \n",
        "    return token_list, tokenizer\n",
        "\n",
        "#형태소 분석기를 따로 저장한 이유는 후에 test 데이터 전처리를 진행할 때 이용해야 되기 때문입니다. \n",
        "#train['new_article'], okt = text_preprocessing(train['content']) "
      ],
      "metadata": {
        "id": "j0bitLTeW8Mm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Vectorization\n",
        "\n",
        "https://dacon.io/competitions/official/235670/codeshare/1841?page=2&dtype=recent"
      ],
      "metadata": {
        "id": "nqHAdWA8Xgjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Vectorization**\n",
        "- NLP를 컴퓨터가 이해할 수 있게 수치로 바꾸는 것\n",
        "- 벡터로 변환된 고유의 토큰들이 모인 집합을 vocabulary 하며 vocabulary가 크면 클수록 학습이 오래 걸림\n"
      ],
      "metadata": {
        "id": "xcs9TVUuXk7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "import re\n",
        "\n",
        "Okt = Okt()\n",
        "\n",
        "sentences = ['자연어 처리는 정말 정말 즐거워.', '즐거운 자연어 처리 다같이 해보자.']\n",
        "tokens = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    sentence = re.sub('[^가-힣a-z]', ' ', sentence) #간단한 전처리\n",
        "    token = (Okt.morphs(sentence)) #형태소 분석기를 이용햔 토큰 나누기\n",
        "    tokens.append(' '.join(token))\n",
        "\n",
        "print(\"형태소 단위로 문장 분리\")\n",
        "print(\"----------------------\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "g1sMMt6OXkjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc19acb3-079b-4081-a045-26542dff866e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "형태소 단위로 문장 분리\n",
            "----------------------\n",
            "['자연어 처리 는 정말 정말 즐거워', '즐거운 자연어 처리 다 같이 해보자']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. One Hot Encoding**"
      ],
      "metadata": {
        "id": "orRRtaDjYNam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow"
      ],
      "metadata": {
        "id": "dgtW79uaawdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9ef35d7-942d-4564-be07-92a071f43e9d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.1)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.8)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(tokens)\n",
        "print(\"각 토큰에게 고유의 정수 부여\")\n",
        "print(\"----------------------\")\n",
        "print(t.word_index) \n",
        "print(\" \")\n",
        "\n",
        "s1=t.texts_to_sequences(tokens)[0] \n",
        "print(\"부여된 정수로 표시된 문장1\")\n",
        "print(\"----------------------\")\n",
        "print(s1)\n",
        "print(\" \")\n",
        "\n",
        "s2=t.texts_to_sequences(tokens)[1]\n",
        "print(\"부여된 정수로 표시된 문장2\")\n",
        "print(\"----------------------\")\n",
        "print(s2)\n",
        "print(\" \")\n",
        "\n",
        "s1_one_hot = to_categorical(s1)\n",
        "print(\"문장1의 one-hot-encoding\")\n",
        "print(\"----------------------\")\n",
        "print(s1_one_hot)\n",
        "print(\" \")\n",
        "\n",
        "s2_one_hot = to_categorical(s2)\n",
        "print(\"문장2의 one-hot-encoding\")\n",
        "print(\"----------------------\")\n",
        "print(s2_one_hot)"
      ],
      "metadata": {
        "id": "Ec9uoa0IYZSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce9d824-af03-4324-bbec-b9c886a043b7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "각 토큰에게 고유의 정수 부여\n",
            "----------------------\n",
            "{'자연어': 1, '처리': 2, '정말': 3, '는': 4, '즐거워': 5, '즐거운': 6, '다': 7, '같이': 8, '해보자': 9}\n",
            " \n",
            "부여된 정수로 표시된 문장1\n",
            "----------------------\n",
            "[1, 2, 4, 3, 3, 5]\n",
            " \n",
            "부여된 정수로 표시된 문장2\n",
            "----------------------\n",
            "[6, 1, 2, 7, 8, 9]\n",
            " \n",
            "문장1의 one-hot-encoding\n",
            "----------------------\n",
            "[[0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]]\n",
            " \n",
            "문장2의 one-hot-encoding\n",
            "----------------------\n",
            "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "하지만 이 방식은 vocabulary 크기가 커짐에 따라 많은 공간을 차지하게 되고 벡터가 굉장히 sparse해지기 때문에 모델에게 좋은 특성을 알려주지 못하는 경우가 대부분"
      ],
      "metadata": {
        "id": "JLY-S_qXYhby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.Count vectorization**"
      ],
      "metadata": {
        "id": "q5imajdQYjIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "vectors = vectorizer.fit_transform(tokens) #여러 개의 문장을 넣어줘야 작동합니다!!\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(vectors.toarray())"
      ],
      "metadata": {
        "id": "CBLdYtMZZGsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d41aff09-0c63-489c-98dc-4185c6a3623d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['같이' '자연어' '정말' '즐거운' '즐거워' '처리' '해보자']\n",
            "[[0 1 2 0 1 1 0]\n",
            " [1 1 0 1 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 벡터화 결과 각 문장은 vocabulary의 인덱스를 기준으로 카운트가 정수로 표시됨.\n",
        "\n",
        "- 즐거운과 즐거워는 같은 의미를 갖는 토큰이지만 okt는 이를 구분해주지 못해서 다른 토큰으로 분리\n",
        "  - 이는 모델에서 같은 의미의 토큰을 다르게 학습할 수 있음을 의미한다."
      ],
      "metadata": {
        "id": "s-KTaW7wZUQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. Tfldf**\n",
        "\n",
        "<핵심>\n",
        "- 단어가 몇 번 등장 했는지에 대한 정보\n",
        "- 어떤 단어가 언급된 문서의 수가 적다면 그 단어는 문서를 분류하는데 있어서 중요한 단어"
      ],
      "metadata": {
        "id": "oC7J3oGvZXH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(min_df=0)\n",
        "tfidf_vectorizer = tfidf.fit_transform(tokens) \n",
        "\n",
        "#tf-idf dictionary    \n",
        "tfidf_dict = tfidf.get_feature_names_out()\n",
        "print(tfidf_dict)\n",
        "print(tfidf_vectorizer.toarray())"
      ],
      "metadata": {
        "id": "JT3fHcmlZ5BU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55522d2a-6011-455f-8b7a-a67bd6532282"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['같이' '자연어' '정말' '즐거운' '즐거워' '처리' '해보자']\n",
            "[[0.         0.29017021 0.81564821 0.         0.4078241  0.29017021\n",
            "  0.        ]\n",
            " [0.49922133 0.35520009 0.         0.49922133 0.         0.35520009\n",
            "  0.49922133]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Padding**\n",
        "\n",
        "- 문장의 길이를 맞춰주기 위해 부족한 길이만큼 0을 채워넣는 것"
      ],
      "metadata": {
        "id": "Lz3qT5rYaILo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**5. 대회적용**"
      ],
      "metadata": {
        "id": "V39i3iipaP6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def text2sequence(train_text, max_len=100):\n",
        "    \n",
        "    tokenizer = Tokenizer() #keras의 vectorizing 함수 호출\n",
        "    tokenizer.fit_on_texts(train_text) #train 문장에 fit\n",
        "    train_X_seq = tokenizer.texts_to_sequences(train_text) #각 토큰들에 정수 부여\n",
        "    vocab_size = len(tokenizer.word_index) + 1 #모델에 알려줄 vocabulary의 크기 계산\n",
        "    print('vocab_size : ', vocab_size)\n",
        "    X_train = pad_sequences(train_X_seq, maxlen = max_len) #설정한 문장의 최대 길이만큼 padding\n",
        "    \n",
        "    return X_train, vocab_size, tokenizer\n",
        "\n",
        "#train_X, vocab_size, vectorizer = text2sequence(train['text'], max_len = 100)"
      ],
      "metadata": {
        "id": "dj-1CP__aSr2"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}