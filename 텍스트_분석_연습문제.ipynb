{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzaeyun/ESAA23_1/blob/main/%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%B6%84%EC%84%9D_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **| 텍스트 분석 연습 문제**\n",
        "\n",
        "- 출처 : 캐글"
      ],
      "metadata": {
        "id": "Yw5mfB-1YfRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Tokenization**\n",
        "\n",
        "In the field of Natural Language Processing, tokenization basically refers to splitting up a larger body of text into smaller lines or words.\n",
        "\n",
        "There are mainly two types of tokenization :\n",
        "\n",
        "- Sentence Tokenization\n",
        "- Word Tokenization"
      ],
      "metadata": {
        "id": "zZBGXubsY6lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import package\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize "
      ],
      "metadata": {
        "id": "zpux756aZRgB",
        "outputId": "6ef6adf2-5f70-43ed-9f6f-b8c9a5d89ba1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample text to perform our operations\n",
        "text = \"Hi, My name is Amartya Nambiar. I am a Computer Science Engineer. My favourite color is black\""
      ],
      "metadata": {
        "id": "-vKDqW1WZcjr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 토큰화\n",
        "\n",
        "sent_tokenize(text=text)"
      ],
      "metadata": {
        "id": "GowligokZeEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce9a8ce-00b8-4854-9dbe-4adff0206470"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi, My name is Amartya Nambiar.',\n",
              " 'I am a Computer Science Engineer.',\n",
              " 'My favourite color is black']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화, 길이 출력\n",
        "words=word_tokenize(text=text)\n",
        "print(words)\n",
        "print(len(words))"
      ],
      "metadata": {
        "id": "pY1VFCkVaDrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d9a5851-4c3f-47a3-c358-020907c6fcd0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', ',', 'My', 'name', 'is', 'Amartya', 'Nambiar', '.', 'I', 'am', 'a', 'Computer', 'Science', 'Engineer', '.', 'My', 'favourite', 'color', 'is', 'black']\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Stopwords & Flushing them**\n",
        "\n",
        "Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence."
      ],
      "metadata": {
        "id": "unjlrUQTaiGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') \n",
        "from nltk.corpus import stopwords  "
      ],
      "metadata": {
        "id": "Rf5p8-7KazcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1ba804f-cdd4-4c2c-c329-91667166f573"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# english stopword 불러오기, 15개만 확인\n",
        "stopwords=nltk.corpus.stopwords.words('english')\n",
        "stopwords[:15]"
      ],
      "metadata": {
        "id": "f8kqXiktbBSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f338812-23ce-44db-f8e2-fd2d7fcf954b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필터링을 통해 text에서 stopword 제거\n",
        "\n",
        "tokens=[]\n",
        "for word in words:\n",
        "  word=word.lower()\n",
        "  if word not in stopwords:\n",
        "    tokens.append(word)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "CTeQujmRbPZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8956dc9c-cf9f-425e-d26a-142be677a153"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hi', ',', 'name', 'amartya', 'nambiar', '.', 'computer', 'science', 'engineer', '.', 'favourite', 'color', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# punctuation('.', ',') 제거\n",
        "\n",
        "new_tokens=[]\n",
        "\n",
        "for word in tokens:\n",
        "  if word not in [',', '.']:\n",
        "    new_tokens.append(word)\n",
        "\n",
        "new_tokens"
      ],
      "metadata": {
        "id": "lxAH2ytMb3TY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb90307-affd-4ca5-cb3d-f38ced4580c7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi',\n",
              " 'name',\n",
              " 'amartya',\n",
              " 'nambiar',\n",
              " 'computer',\n",
              " 'science',\n",
              " 'engineer',\n",
              " 'favourite',\n",
              " 'color',\n",
              " 'black']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Stemming**\n",
        "\n",
        "Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat.\n",
        "\n",
        "There are mainly two widely used Stemmer Algorithms:\n",
        "\n",
        "- Porter Stemmer (we'll work on this)\n",
        "- Lancaster Stem"
      ],
      "metadata": {
        "id": "yEAlzfMhcBcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "EQQuNe0McNBg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ps 객체 생성 후 stemming , example 최소 3개 임의 생성 후 시도해보기\n",
        "\n",
        "ps=PorterStemmer()\n",
        "example1= ['helps', 'helping', 'helped']\n",
        "example2= ['works', 'working', 'worked']\n",
        "example3= ['amuses', 'amusing', 'amused']\n",
        "\n",
        "print(ps.stem(example1[0]),ps.stem(example1[1]),ps.stem(example1[2]))\n",
        "print(ps.stem(example2[0]),ps.stem(example2[1]),ps.stem(example2[2]))\n",
        "print(ps.stem(example3[0]),ps.stem(example3[1]),ps.stem(example3[2]))"
      ],
      "metadata": {
        "id": "hp_atqYwdkR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "649684fc-fc60-4e21-a7fe-54255a8259b2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "help help help\n",
            "work work work\n",
            "amus amus amus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('happiness') # but it isn't always the best choice"
      ],
      "metadata": {
        "id": "mwMDJ3ZaduyK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bdd8e3b9-2c42-4725-94f6-8cc4b989ab53"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Parts of Speech**\n",
        "\n",
        "To know what is the context of a particular word\n",
        "\n",
        "For example : Shyam is a Proper Noun, Desk is a Noun and Happy is an adjective."
      ],
      "metadata": {
        "id": "71eO1YE1dwBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.corpus import movie_reviews\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "text = movie_reviews.raw(\"neg/cv954_19932.txt\") "
      ],
      "metadata": {
        "id": "S2iVOPi4eEKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae1dbff8-5455-450d-b829-5ca7323b0358"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "N1K9YczmH-JC",
        "outputId": "d600daff-9a4c-4b0c-90f7-a73d9853b154"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a new entry in the \" revisionist history \" genre of filmmaking , dick suggests that two not-too-bright teenage girls are the cause of the uncovering of the nation\\'s biggest presidential scandal . \\nkirsten dunst and michelle williams star betsy and arlene , who while trying to deliver a fan letter from arlene\\'s watergate hotel room , accidentally stumble across g . gordon liddy ( played dead-on by harry shearer ) and the infamous break-in . \\nwhen they recognize liddy later on during a white house field trip , they are ushered into a conference room , questioned as to what they know , and leave as official presidential dog walkers . \\nthe girls manage to unwittingly uncover every bit of the watergate scandal while performing their duties , but have no clue as to what they are getting involved with . \\nwhen they discover that nixon ( another dead-on performance by dan hedaya , who actually favors nixon slightly , unlike anthony hopkins ) has been abusive to checkers , the presidential dog , thanks to the conversations that he always recorded , they quit and become disillusioned . \\nduring a prank phone call the girls make to woodward and bernstein , events are set into motion that eventually lead to the president\\'s resignation . \\nthis film starts off promisingly with an aged woodward and bernstein arguing with each other on an obvious larry king-type talk show ( featuring a cameo by french stewart ) about revealing the identity of \" deep throat \" . \\nfrom there , we are subjected to bodily function humor and just about every bad \" dick \" joke one can derive from this type of supposed comedy . \\nat one point , the girls are having to scream over a high school band playing on the steps of the lincoln memorial . \\nthe band manages to stop right as dunst screams \" you have to stop letting dick run your life ! \" \\nmuch to the horror of everyone standing within earshot . \\nseveral other variations on this wordplay surface all throughout the film . \\nif this movie had been smarter i would have been less likely to fault it\\'s juvenile bathroom humor , but it\\'s not . \\nthe film was apparently made for relatively younger people because every major player in the watergate scandal is introduced and shoved down the audience\\'s throat in the least subtle way possible . \\ni don\\'t recall oliver stone\\'s nixon having to pander to it\\'s audience , but of course that film wasn\\'t a comedy aimed squarely at a 13-20 year-old film going audience . \\nthe only redeeming thing about this movie is it\\'s remarkable supporting cast . \\ni wanted to see more of ferrell and mcculloch\\'s woodward and bernstein . \\nthose two characters are the sole basis for my rating . \\ni wish they had been given more screen time , but unfortunately , they are only relegated to the final half-hour . \\ntheir constant bickering and fighting over trying to get the story are a major highlight , especially mcculloch\\'s constant thwarting of ferrell\\'s attempts to gather information from the girls ( who , in the course of the narrative are revealed as deep throat , so named thanks to an ill planned trip to a porno theater by betsy\\'s brother ) . \\nthe other members of the cast are excellent in their portrayals of their particular characters , but are given nothing to work with . \\ni\\'d like to see the same cast portray these characters in a script more suited towards their comedic abilities . \\nas for the two leads , dunst and williams can definitely do better . \\nthey come off as what could best be described as romy and michele : the early years in this particular film , a highly dubious distinction at best . \\nstay through the first half of the end credits though , to see an interesting scene involving dunst and williams suggestively sucking on lollipops emblazoned with the title of the movie . \\nan excellent idea marred by poor execution , dick could have been a great movie . \\nless of the juvenile humor and more of the smarter comedy displayed by the woodward and bernstein scenes , could have made this film a wonderful satire of the nixon presidency as seen through the eyes of two naive fifteen year olds . \\nas it stands though , dick offers nothing but what filmmaker kevin smith so accurately defines as \" dick and poopie \" jokes . \\nand that , to me , does not make a funny movie . \\n[pg-13] \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply pos_tag(), print result\n",
        "word_tokenize(text=text)\n",
        "\n",
        "tokens=[]\n",
        "for word in words:\n",
        "  word=word.lower()\n",
        "  if word not in stopwords:\n",
        "    tokens.append(word)\n",
        "\n",
        "new_tokens=[]\n",
        "\n",
        "for word in tokens:\n",
        "  if word not in [',', '.']:\n",
        "    new_tokens.append(word)\n",
        "\n",
        "print(pos_tag(new_tokens))"
      ],
      "metadata": {
        "id": "vcWgmScAedLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c89a7d-3579-405f-98d5-aeec3af355a8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hi', 'NN'), ('name', 'NN'), ('amartya', 'NN'), ('nambiar', 'JJ'), ('computer', 'NN'), ('science', 'NN'), ('engineer', 'NN'), ('favourite', 'NN'), ('color', 'NN'), ('black', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Lemmatization**\n",
        "\n",
        "PorterStemmer class chops off the suffixes from the word but this isn't the best thing to apply to clean our data.\n",
        "\n",
        "Stemming technique only looks at the form of the word whereas Lemmatization technique looks at the meaning of the word. It means after applying lemmatization, we will always get a valid word."
      ],
      "metadata": {
        "id": "NrFml-IJepQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import package\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "lemma=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "xWF0Ibznetk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f64f2684-e795-448b-9364-d0caa310595d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatize 'believes', 'happiness'\n",
        "\n",
        "lemma.lemmatize('believes', 'v')"
      ],
      "metadata": {
        "id": "C-abWCqffiwP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6c4c9e88-6387-4868-d968-7db5b0873b97"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'believe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma.lemmatize('happiness', 'a')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iy2EQAcRHlc7",
        "outputId": "6d9fea6c-9955-42a5-986f-5a6bbf4ee9c1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happiness'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}